{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation for Evaluation\n",
    "\n",
    "* MeanIOU: Image Segmentation에서 많이 쓰이는 evaluation measure\n",
    "* tf.version 1.12 API: [`tf.metrics.mean_iou`](https://www.tensorflow.org/api_docs/python/tf/metrics/mean_iou)\n",
    "  * `tf.enable_eager_execution()`이 작동하지 않음\n",
    "  * 따라서 예전 방식대로 `tf.Session()`을 이용하여 작성하거나 아래와 같이 2.0 version으로 작성하여야 함\n",
    "* tf.version 2.0 API: [`tf.keras.metrics.MeanIoU`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics/MeanIoU)\n",
    "\n",
    "* 지금 이 코드는 `version 1` 코드로 `tf.Session()`을 이용하여 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you necessary\n",
    "\n",
    "#from google.colab import auth\n",
    "#auth.authenticate_user()\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_colab = True\n",
    "assert use_colab in [True, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import models\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_colab:\n",
    "  dataset_dir = './gdrive/My Drive/datasets/sd_train'\n",
    "else:\n",
    "  dataset_dir = '../../datasets/sd_train'\n",
    "img_dir = os.path.join(dataset_dir, \"train\")\n",
    "label_dir = os.path.join(dataset_dir, \"train_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_filenames = [os.path.join(img_dir, filename) for filename in os.listdir(img_dir)]\n",
    "x_train_filenames.sort()\n",
    "y_train_filenames = [os.path.join(label_dir, filename) for filename in os.listdir(label_dir)]\n",
    "y_train_filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_filenames, x_test_filenames, y_train_filenames, y_test_filenames = \\\n",
    "                    train_test_split(x_train_filenames, y_train_filenames, test_size=0.2, random_state=219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_examples = len(x_train_filenames)\n",
    "num_test_examples = len(x_test_filenames)\n",
    "\n",
    "print(\"Number of training examples: {}\".format(num_train_examples))\n",
    "print(\"Number of test examples: {}\".format(num_test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our input pipeline with `tf.data`\n",
    "### Set up test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "image_size = 64\n",
    "img_shape = (image_size, image_size, 3)\n",
    "batch_size = 60 # all test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_pathnames(fname, label_path):\n",
    "  # We map this function onto each pathname pair\n",
    "  img_str = tf.io.read_file(fname)\n",
    "  img = tf.image.decode_bmp(img_str, channels=3)\n",
    "\n",
    "  label_img_str = tf.io.read_file(label_path)\n",
    "  label_img = tf.image.decode_bmp(label_img_str, channels=1)\n",
    "  \n",
    "  resize = [image_size, image_size]\n",
    "  img = tf.image.resize_images(img, resize)\n",
    "  label_img = tf.image.resize_images(label_img, resize)\n",
    "  \n",
    "  scale = 1 / 255.\n",
    "  img = tf.to_float(img) * scale\n",
    "  label_img = tf.to_float(label_img) * scale\n",
    "  \n",
    "  return img, label_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_dataset(filenames,\n",
    "                         labels,\n",
    "                         threads=5,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True):\n",
    "  num_x = len(filenames)\n",
    "  # Create a dataset from the filenames and labels\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "  # Map our preprocessing function to every element in our dataset, taking\n",
    "  # advantage of multithreading\n",
    "  dataset = dataset.map(_process_pathnames, num_parallel_calls=threads)\n",
    "  \n",
    "  if shuffle:\n",
    "    dataset = dataset.shuffle(num_x * 10)\n",
    "  \n",
    "  dataset = dataset.batch(batch_size)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = get_baseline_dataset(x_test_filenames,\n",
    "                                    y_test_filenames,\n",
    "                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, num_filters):\n",
    "  encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
    "  encoder = layers.BatchNormalization()(encoder)\n",
    "  encoder = layers.Activation('relu')(encoder)\n",
    "  return encoder\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "  encoder = conv_block(input_tensor, num_filters)\n",
    "  encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
    "  \n",
    "  return encoder_pool, encoder\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
    "  decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
    "  decoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
    "  decoder = layers.BatchNormalization()(decoder)\n",
    "  decoder = layers.Activation('relu')(decoder)\n",
    "  decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "  decoder = layers.BatchNormalization()(decoder)\n",
    "  decoder = layers.Activation('relu')(decoder)\n",
    "  return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=img_shape)\n",
    "# 256\n",
    "\n",
    "encoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
    "encoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
    "encoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
    "encoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
    "\n",
    "center = conv_block(encoder3_pool, 512) # center\n",
    "\n",
    "decoder3 = decoder_block(center, encoder3, 256) # 32\n",
    "decoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
    "decoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
    "decoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
    "\n",
    "outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore using Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = train_dir\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "\n",
    "# Restore the latest checkpoint\n",
    "status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the test dataset and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "status.initialize_or_restore(sess)\n",
    "tf.logging.info('Start Session.')\n",
    "\n",
    "test_iter = test_dataset.make_one_shot_iterator()\n",
    "images, targets = test_iter.get_next()\n",
    "\n",
    "mean_iou, mean_iou_op = tf.metrics.mean_iou(labels=tf.to_int32(tf.round(model(images))),\n",
    "                                            predictions=tf.to_int32(tf.round(targets)),\n",
    "                                            num_classes=2,\n",
    "                                            name='mean_iou')\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "sess.run(mean_iou_op)\n",
    "print(\"mean iou:\", sess.run(mean_iou))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
