{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation\n",
    "  \n",
    "#### Task\n",
    "* GIANA dataset으로 위내시경 이미지에서 용종을 segmentation 해보자\n",
    "* Image size: 256으로 변경하여 수행 (baseline code는 `image_size`: 64)\n",
    "* 밑에 제시된 여러가지 시도를 해보자\n",
    "* This code is borrowed from [TensorFlow tutorials/Image Segmentation](https://github.com/tensorflow/models/blob/master/samples/outreach/blogs/segmentation_blogpost/image_segmentation.ipynb) which is made of `tf.keras.layers` and `tf.enable_eager_execution()`.\n",
    "* You can see the detail description [tutorial link](https://github.com/tensorflow/models/blob/master/samples/outreach/blogs/segmentation_blogpost/image_segmentation.ipynb)  \n",
    "\n",
    "#### Dataset\n",
    "* I use below dataset instead of [carvana-image-masking-challenge dataset](https://www.kaggle.com/c/carvana-image-masking-challenge/rules) in TensorFlow Tutorials which is a kaggle competition dataset.\n",
    "  * carvana-image-masking-challenge dataset: Too large dataset (14GB)\n",
    "* [Gastrointestinal Image ANAlys Challenges (GIANA)](https://giana.grand-challenge.org) Dataset (345MB)\n",
    "  * Train data: 300 images with RGB channels (bmp format)\n",
    "  * Train lables: 300 images with 1 channels (bmp format)\n",
    "  * Image size: 574 x 500\n",
    "\n",
    "#### Baseline code\n",
    "* Dataset: train, test로 split\n",
    "* Input data shape: (`batch_size`, 64, 64, 3)\n",
    "* Output data shape: (`batch_size`, 64, 64, 1)\n",
    "* Architecture: \n",
    "  * 간단한 U-Net 구조\n",
    "  * [`tf.keras.layers`](https://www.tensorflow.org/api_docs/python/tf/keras/layers) 사용\n",
    "* Training\n",
    "  * `tf.data.Dataset` 사용\n",
    "  * `tf.GradientTape()` 사용 for weight update\n",
    "* Evaluation\n",
    "  * MeanIOU: Image Segmentation에서 많이 쓰이는 evaluation measure\n",
    "  * tf.version 1.12 API: [`tf.metrics.mean_iou`](https://www.tensorflow.org/api_docs/python/tf/metrics/mean_iou)\n",
    "    * `tf.enable_eager_execution()`이 작동하지 않음\n",
    "    * 따라서 예전 방식대로 `tf.Session()`을 이용하여 작성하거나 아래와 같이 2.0 version으로 작성하여야 함\n",
    "  * tf.version 2.0 API: [`tf.keras.metrics.MeanIoU`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics/MeanIoU)\n",
    "\n",
    "#### Try some techniques\n",
    "* Change model architectures (Custom model)\n",
    "  * Try another models (DeepLAB, Hourglass, Encoder-Decoder 모델)\n",
    "* Data augmentation\n",
    "* Various regularization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you necessary\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_colab = True\n",
    "assert use_colab in [True, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import models\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the files \n",
    "Since this tutorial will be using a dataset from [Giana Dataset](https://giana.grand-challenge.org/Dates/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately you cannot downlaod GIANA dataset from website\n",
    "# So I upload zip file on my dropbox\n",
    "# if you want to download from my dropbox uncomment below  \n",
    "# if use_colab:\n",
    "#   DATASET_PATH='./gdrive/My Drive/datasets'\n",
    "# else:\n",
    "#   DATASET_PATH='../../datasets'\n",
    "# !wget https://goo.gl/mxikqa\n",
    "# !mv mxikqa sd_train.zip\n",
    "# !unzip sd_train.zip\n",
    "# if not os.path.isdir(DATASET_PATH):\n",
    "#   os.makedirs(DATASET_PATH)\n",
    "# shutil.move(os.path.join('sd_train'), os.path.join(DATASET_PATH))\n",
    "# !rm sd_train.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy dataset to Google drvie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_colab:\n",
    "  dataset_dir = './gdrive/My Drive/datasets/sd_train'\n",
    "else:\n",
    "  dataset_dir = '../../datasets/sd_train'\n",
    "img_dir = os.path.join(dataset_dir, \"train\")\n",
    "label_dir = os.path.join(dataset_dir, \"train_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_filenames = [os.path.join(img_dir, filename) for filename in os.listdir(img_dir)]\n",
    "x_train_filenames.sort()\n",
    "y_train_filenames = [os.path.join(label_dir, filename) for filename in os.listdir(label_dir)]\n",
    "y_train_filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_filenames, x_test_filenames, y_train_filenames, y_test_filenames = \\\n",
    "                    train_test_split(x_train_filenames, y_train_filenames, test_size=0.2, random_state=219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_examples = len(x_train_filenames)\n",
    "num_test_examples = len(x_test_filenames)\n",
    "\n",
    "print(\"Number of training examples: {}\".format(num_train_examples))\n",
    "print(\"Number of test examples: {}\".format(num_test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "Let's take a look at some of the examples of different images in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_num = 5\n",
    "\n",
    "r_choices = np.random.choice(num_train_examples, display_num)\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "for i in range(0, display_num * 2, 2):\n",
    "  img_num = r_choices[i // 2]\n",
    "  x_pathname = x_train_filenames[img_num]\n",
    "  y_pathname = y_train_filenames[img_num]\n",
    "  \n",
    "  plt.subplot(display_num, 2, i + 1)\n",
    "  plt.imshow(Image.open(x_pathname))\n",
    "  plt.title(\"Original Image\")\n",
    "  \n",
    "  example_labels = Image.open(y_pathname)\n",
    "  label_vals = np.unique(example_labels)\n",
    "  \n",
    "  plt.subplot(display_num, 2, i + 2)\n",
    "  plt.imshow(example_labels)\n",
    "  plt.title(\"Masked Image\")\n",
    "  \n",
    "plt.suptitle(\"Examples of Images and their Masks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "Let’s begin by setting up some parameters. We’ll standardize and resize all the shapes of the images. We’ll also set up some training parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "image_size = 64\n",
    "img_shape = (image_size, image_size, 3)\n",
    "batch_size = 8\n",
    "max_epochs = 2\n",
    "print_steps = 10\n",
    "save_epochs = 1\n",
    "\n",
    "if use_colab:\n",
    "  train_dir='./gdrive/My Drive/train_ckpt/segmentation/exp1'\n",
    "  if not os.path.isdir(train_dir):\n",
    "    os.makedirs(train_dir)\n",
    "else:\n",
    "  train_dir = 'train/exp1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our input pipeline with `tf.data`\n",
    "Since we begin with filenames, we will need to build a robust and scalable data pipeline that will play nicely with our model. If you are unfamiliar with **tf.data** you should check out my other tutorial introducing the concept! \n",
    "\n",
    "### Our input pipeline will consist of the following steps:\n",
    "1. Read the bytes of the file in from the filename - for both the image and the label. Recall that our labels are actually images with each pixel annotated as car or background (1, 0). \n",
    "2. Decode the bytes into an image format\n",
    "3. Apply image transformations: (optional, according to input parameters)\n",
    "  * `resize` - Resize our images to a standard size (as determined by eda or computation/memory restrictions)\n",
    "    * The reason why this is optional is that U-Net is a fully convolutional network (e.g. with no fully connected units) and is thus not dependent on the input size. However, if you choose to not resize the images, you must use a batch size of 1, since you cannot batch variable image size together\n",
    "    * Alternatively, you could also bucket your images together and resize them per mini-batch to avoid resizing images as much, as resizing may affect your performance through interpolation, etc.\n",
    "  * `hue_delta` - Adjusts the hue of an RGB image by a random factor. This is only applied to the actual image (not our label image). The `hue_delta` must be in the interval `[0, 0.5]` \n",
    "  * `horizontal_flip` - flip the image horizontally along the central axis with a 0.5 probability. This transformation must be applied to both the label and the actual image. \n",
    "  * `width_shift_range` and `height_shift_range` are ranges (as a fraction of total width or height) within which to randomly translate the image either horizontally or vertically. This transformation must be applied to both the label and the actual image. \n",
    "  * `rescale` - rescale the image by a certain factor, e.g. 1/ 255.\n",
    "4. Shuffle the data, repeat the data (so we can iterate over it multiple times across epochs), batch the data, then prefetch a batch (for efficiency).\n",
    "\n",
    "It is important to note that these transformations that occur in your data pipeline must be symbolic transformations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we do these image transformations?\n",
    "This is known as **data augmentation**. Data augmentation \"increases\" the amount of training data by augmenting them via a number of random transformations. During training time, our model would never see twice the exact same picture. This helps prevent [overfitting](https://developers.google.com/machine-learning/glossary/#overfitting) and helps the model generalize better to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing each pathname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_pathnames(fname, label_path):\n",
    "  # We map this function onto each pathname pair\n",
    "  img_str = tf.read_file(fname)\n",
    "  img = tf.image.decode_bmp(img_str, channels=3)\n",
    "\n",
    "  label_img_str = tf.read_file(label_path)\n",
    "  label_img = tf.image.decode_bmp(label_img_str, channels=1)\n",
    "  \n",
    "  resize = [image_size, image_size]\n",
    "  img = tf.image.resize_images(img, resize)\n",
    "  label_img = tf.image.resize_images(label_img, resize)\n",
    "  \n",
    "  scale = 1 / 255.\n",
    "  img = tf.to_float(img) * scale\n",
    "  label_img = tf.to_float(label_img) * scale\n",
    "  \n",
    "  return img, label_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_dataset(filenames,\n",
    "                         labels,\n",
    "                         threads=5,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True):\n",
    "  num_x = len(filenames)\n",
    "  # Create a dataset from the filenames and labels\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "  # Map our preprocessing function to every element in our dataset, taking\n",
    "  # advantage of multithreading\n",
    "  dataset = dataset.map(_process_pathnames, num_parallel_calls=threads)\n",
    "  \n",
    "  if shuffle:\n",
    "    dataset = dataset.shuffle(num_x * 10)\n",
    "  \n",
    "  dataset = dataset.batch(batch_size)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up train and test datasets\n",
    "Note that we apply image augmentation to our training dataset but not our validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_baseline_dataset(x_train_filenames,\n",
    "                                     y_train_filenames)\n",
    "test_dataset = get_baseline_dataset(x_test_filenames,\n",
    "                                    y_test_filenames,\n",
    "                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_dataset.take(1):\n",
    "  # Running next element in our graph will produce a batch of images\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  img = images[0]\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.imshow(img)\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.imshow(labels[0, :, :, 0])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "We'll build the U-Net model. U-Net is especially good with segmentation tasks because it can localize well to provide high resolution segmentation masks. In addition, it works well with small datasets and is relatively robust against overfitting as the training data is in terms of the number of patches within an image, which is much larger than the number of training images itself. Unlike the original model, we will add batch normalization to each of our blocks. \n",
    "\n",
    "The Unet is built with an encoder portion and a decoder portion. The encoder portion is composed of a linear stack of [`Conv`](https://developers.google.com/machine-learning/glossary/#convolution), `BatchNorm`, and [`Relu`](https://developers.google.com/machine-learning/glossary/#ReLU) operations followed by a [`MaxPool`](https://developers.google.com/machine-learning/glossary/#pooling). Each `MaxPool` will reduce the spatial resolution of our feature map by a factor of 2. We keep track of the outputs of each block as we feed these high resolution feature maps with the decoder portion. The Decoder portion is comprised of UpSampling2D, Conv, BatchNorm, and Relus. Note that we concatenate the feature map of the same size on the decoder side. Finally, we add a final Conv operation that performs a convolution along the channels for each individual pixel (kernel size of (1, 1)) that outputs our final segmentation mask in grayscale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Keras Functional API\n",
    "The Keras functional API is used when you have multi-input/output models, shared layers, etc. It's a powerful API that allows you to manipulate tensors and build complex graphs with intertwined datastreams easily. In addition it makes layers and models both callable on tensors.\n",
    "\n",
    "* To see more examples check out the [get started guide](https://keras.io/getting-started/functional-api-guide/).\n",
    "\n",
    "We'll build these helper functions that will allow us to ensemble our model block operations easily and simply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, num_filters):\n",
    "  encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
    "  encoder = layers.BatchNormalization()(encoder)\n",
    "  encoder = layers.Activation('relu')(encoder)\n",
    "  return encoder\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "  encoder = conv_block(input_tensor, num_filters)\n",
    "  encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
    "  \n",
    "  return encoder_pool, encoder\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
    "  decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
    "  decoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
    "  decoder = layers.BatchNormalization()(decoder)\n",
    "  decoder = layers.Activation('relu')(decoder)\n",
    "  decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "  decoder = layers.BatchNormalization()(decoder)\n",
    "  decoder = layers.Activation('relu')(decoder)\n",
    "  return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=img_shape)\n",
    "# 256\n",
    "\n",
    "encoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
    "encoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
    "encoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
    "encoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
    "\n",
    "center = conv_block(encoder3_pool, 512) # center\n",
    "\n",
    "decoder3 = decoder_block(center, encoder3, 256) # 32\n",
    "decoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
    "decoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
    "decoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
    "\n",
    "outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model (UNet)\n",
    "Using functional API, you must define your model by specifying the inputs and outputs associated with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining custom metrics and loss functions\n",
    "\n",
    "Defining loss and metric functions are simple with Keras. Simply define a function that takes both the True labels for a given example and the Predicted labels for the same given example.\n",
    "\n",
    "Dice loss is a metric that measures overlap. More info on optimizing for Dice coefficient (our dice loss) can be found in the [paper](http://campar.in.tum.de/pub/milletari2016Vnet/milletari2016Vnet.pdf), where it was introduced.\n",
    "\n",
    "We use dice loss here because it performs better at class imbalanced problems by design. In addition, maximizing the dice coefficient and IoU metrics are the actual objectives and goals of our segmentation task. Using cross entropy is more of a proxy which is easier to maximize. Instead, we maximize our objective directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(y_true, y_pred):\n",
    "  smooth = 1.\n",
    "  # Flatten\n",
    "  y_true_f = tf.reshape(y_true, [-1])\n",
    "  y_pred_f = tf.reshape(y_pred, [-1])\n",
    "  intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "  score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "  loss = 1 - dice_coeff(y_true, y_pred)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll use a specialized loss function that combines binary cross entropy and our dice loss. This is based on [individuals who competed within this competition obtaining better results empirically](https://www.kaggle.com/c/carvana-image-masking-challenge/discussion/40199). Try out your own custom losses to measure performance (e.g. bce + log(dice_loss), only bce, etc.)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_dice_loss(y_true, y_pred):\n",
    "  loss = tf.reduce_mean(losses.binary_crossentropy(y_true, y_pred)) + dice_loss(y_true, y_pred)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = train_dir\n",
    "if not tf.gfile.Exists(checkpoint_dir):\n",
    "  tf.gfile.MakeDirs(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model\n",
    "Training your model with tf.data involves simply providing the model's fit function with your training/validation dataset, the number of steps, and epochs.\n",
    "\n",
    "We also include a Model callback, ModelCheckpoint that will save the model to disk after each epoch. We configure it such that it only saves our highest performing model. Note that saving the model capture more than just the weights of the model: by default, it saves the model architecture, weights, as well as information about the training process such as the state of the optimizer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.logging.info('Start Training.')\n",
    "\n",
    "# save loss values for plot\n",
    "loss_history = []\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "for epoch in range(max_epochs):\n",
    "  \n",
    "  for images, labels in train_dataset:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = model(images)\n",
    "      loss = bce_dice_loss(labels, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.variables),\n",
    "                              global_step=global_step)\n",
    "    \n",
    "    epochs = global_step.numpy() * batch_size / float(num_train_examples)\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    if global_step.numpy() % print_steps == 0:\n",
    "      clear_output(wait=True)\n",
    "      examples_per_sec = batch_size  / float(duration)\n",
    "      print(\"Epochs: {:.2f} global_step: {} loss: {:.3f} ({:.2f} examples/sec; {:.3f} sec/batch)\".format(\n",
    "                epochs, global_step.numpy(), loss, examples_per_sec, duration))\n",
    "\n",
    "      loss_history.append([epochs, loss])\n",
    "\n",
    "      # print sample image\n",
    "      for test_images, test_labels in test_dataset.take(1):\n",
    "        predictions = model(test_images)\n",
    "        \n",
    "      plt.figure(figsize=(10, 20))\n",
    "      plt.subplot(1, 3, 1)\n",
    "      plt.imshow(test_images[0,: , :, :])\n",
    "      plt.title(\"Input image\")\n",
    "  \n",
    "      plt.subplot(1, 3, 2)\n",
    "      plt.imshow(test_labels[0, :, :, 0])\n",
    "      plt.title(\"Actual Mask\")\n",
    "      \n",
    "      plt.subplot(1, 3, 3)\n",
    "      plt.imshow(predictions[0, :, :, 0])\n",
    "      plt.title(\"Predicted Mask\")\n",
    "      plt.show()\n",
    "\n",
    "  # saving (checkpoint) the model periodically\n",
    "  if (epoch+1) % save_epochs == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "tf.logging.info('complete training...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = np.asarray(loss_history)\n",
    "plt.plot(loss_history[:,0], loss_history[:,1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
