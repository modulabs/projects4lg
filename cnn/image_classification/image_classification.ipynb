{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "### Task\n",
    "* Overftting을 피하며, accuracy를 높혀 보자\n",
    "* Image size: 224 또는 299로 변경하여 수행 (baseline code는 `image_size`: 150)\n",
    "* 밑에 제시된 여러가지 시도를 해보자\n",
    "\n",
    "### Dataset\n",
    "* [Google flower datasets](https://github.com/tensorflow/models/blob/master/research/inception/inception/data/download_and_preprocess_flowers.sh)\n",
    "* 5개의 클래스(daisy, dandelion, roses, sunflowers, tulips)로 이루어진 꽃 이미지 데이터를 분류\n",
    "\n",
    "### Baseline code\n",
    "* Dataset: train, validation, test로 split\n",
    "* Input data shape: (`batch_size`, 150, 150, 3)\n",
    "* Output data shape: (`batch_size`, `num_classes`=5)\n",
    "* Architecture: \n",
    "  * `Conv2D` (x3) - `Dense` - `Softmax`\n",
    "  * [`tf.keras.layers`](https://www.tensorflow.org/api_docs/python/tf/keras/layers) 사용\n",
    "* Training\n",
    "  * `model.fit_generator` 사용\n",
    "  * `tf.keras.preprocessing.image.ImageDataGenerator` 사용 for data augmentation\n",
    "* Evaluation\n",
    "  * `model.evaluate_generator` 사용 for test dataset\n",
    "\n",
    "### Try some techniques\n",
    "* Change model architectures (Custom model)\n",
    "  * Or use pretrained models\n",
    "* Data augmentation\n",
    "* Various regularization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "이미지 분류 프로그램을 만들려면 먼저 데이터 세트를 다운로드해야합니다. 우리가 사용하는 데이터 세트는 Google Flower dataset 입니다. 먼저 dataset을 다운로드 한 다음 `'../../datasets/'` 디렉토리에 저장하고 압축을 풉니다.\n",
    "\n",
    "데이터 구조는 아래와 같습니다.\n",
    "\n",
    "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
    "<b>flower</b>\n",
    "|__ <b>train</b>\n",
    "    |____ <b>daisy</b>: [5547758_eea9edfd54_n.jpg, 5673551_01d1ea993e_n.jpg, ....]\n",
    "    |____ <b>dandelion</b>: [7355522_b66e5d3078_m.jpg, 10443973_aeb97513fc_m.jpg, ...]\n",
    "    |____ <b>...</b>: [...]\n",
    "|__ <b>validation</b>\n",
    "    |____ <b>daisy</b>: [705422469_ffa28c566d.jpg, 721595842_bacd80a6ac.jpg, ...]\n",
    "    |____ <b>dandelion</b>: [7355522_b66e5d3078_m.jpg, 751941983_58e1ae3957_m.jpg, ...]\n",
    "    |____ <b>...</b>: [...]\n",
    "|__ <b>test</b>\n",
    "    |____ <b>daisy</b>: [99306615_739eb94b9e_m.jpg, 813445367_187ecf080a_n.jpg, ...]\n",
    "    |____ <b>dandelion</b>: [8181477_8cb77d2e0f_n.jpg, 8223949_2928d3f6f6_n.jpg, ...]\n",
    "    |____ <b>...</b>: [...]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I upload zip file on my dropbox\n",
    "# if you want to download from my dropbox uncomment below\n",
    "# !wget https://goo.gl/motrG4\n",
    "# !mv motrG4 flower.zip\n",
    "# !unzip flower.zip\n",
    "# !mkdir ../../datasets\n",
    "# !mv flower ../../datasets\n",
    "# !rm flower.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../../datasets/flower/'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "print(train_dir)\n",
    "print(validation_dir)\n",
    "print(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = sorted(os.listdir(train_dir))\n",
    "for name in class_name:\n",
    "  print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding out data\n",
    "\n",
    "train, validation 및 test 디렉토리에서 얼마나 많은 이미지가 있는지 살펴 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 0\n",
    "num_val = 0\n",
    "num_test = 0\n",
    "for name in class_name:\n",
    "  train_path = os.path.join(train_dir, name)\n",
    "  val_path = os.path.join(validation_dir, name)\n",
    "  test_path = os.path.join(test_dir, name)\n",
    "  print(\"Number of {} class: for train: {} / for validation: {} / for test: {}\".format(name,\n",
    "                                                                len(os.listdir(train_path)),\n",
    "                                                                len(os.listdir(val_path)),\n",
    "                                                                len(os.listdir(train_path))))\n",
    "  num_train += len(os.listdir(train_path))\n",
    "  num_val += len(os.listdir(val_path))\n",
    "  num_test += len(os.listdir(test_path))\n",
    "\n",
    "print('--------')\n",
    "print(\"Total training images:\", num_train)\n",
    "print(\"Total validation images:\", num_val)\n",
    "print(\"Total test images:\", num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Model Parameters\n",
    "\n",
    "For convenience, let us set up variables that will be later used while pre-processing our dataset and training our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 15\n",
    "IMG_SHAPE = 150  # Our training data consists of images with width of 150 pixels and height of 150 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images should be formatted into appropriately pre-processed floating point tensors before being fed into the network. The steps involving preparing these images are:\n",
    "\n",
    "1. Read images from the disk\n",
    "2. Decode contents of these images and convert it into proper grid format as per their RGB content\n",
    "3. Convert them into floating point tensors\n",
    "4. Rescale the tensors from values between 0 and 255 to values between 0 and 1, as neural networks prefer to deal with small input values.\n",
    "\n",
    "Fortunately, all these tasks can be done using a single class provided in **tf.keras** preprocessing module, called **ImageDataGenerator**. Not only it can read images from the disks and preprocess images into proper tensors, but it will also set up generators that will turn these images into batches of tensors, which will be very helpful while training our network as we need to pass our input to the network in the form of batches.\n",
    "\n",
    "We can easily set up this using a couple of lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\n",
    "val_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data\n",
    "test_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining our generators for training and validation images, **flow_from_directory** method will load images from the disk and will apply rescaling and will resize them into required dimensions using single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                           directory=train_dir,\n",
    "                                                           # Its usually best practice to shuffle the training data\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(IMG_SHAPE,IMG_SHAPE), #(150,150)\n",
    "                                                           class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_gen = val_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                       directory=validation_dir,\n",
    "                                                       target_size=(IMG_SHAPE,IMG_SHAPE), #(150,150)\n",
    "                                                       class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_gen = test_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                         directory=test_dir,\n",
    "                                                         target_size=(IMG_SHAPE,IMG_SHAPE), #(150,150)\n",
    "                                                         class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Training images\n",
    "\n",
    "We can visualize our training images by using following lines of code which will first extract a batch of images from training generator, which is 32 images in our case and then we will plot 5 of them using **matplotlib**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_training_images, _ = next(train_data_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**next** function returns a batch from the dataset. The return value of **next** function is in form of (x_train, y_train) where x_train is training features and y_train, its labels. We are discarding the labels in above situation because we only want to visualize our training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotImages(sample_training_images[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation\n",
    "\n",
    "The model consists of 3 convolution blocks with max pool layer in each of them. We have a fully connected layer with 512 units on top of it, which is activated by **relu** activation function. Model will output class probabilities based on categorical classification which is done by **softmax** activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_SHAPE,IMG_SHAPE, 3,))) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "We will use **ADAM** optimizer as our choice of optimizer for this task and **categorical cross entropy** function as a loss function. We would also like to look at training and validation accuracy on each epoch as we train our network, for that we are passing it in the metrics argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', \n",
    "#               loss='categorical_crossentropy', \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary\n",
    "Let's look at all the layers of our network using **summary** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Its time we train our network. We will use **fit_generator** function to train our network instead of **fit** function, as we are using **ImageDataGenerator** class to generate batches of training and validation data for our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=int(np.ceil(num_train / float(batch_size))),\n",
    "    epochs=epochs,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=int(np.ceil(num_val / float(batch_size)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing results of the training\n",
    "\n",
    "Let us now visualize the results we get after training our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plots, training accuracy and validation accuracy are off by large margin and our model has achieved only around **70%** accuracy on the validation set, let us analyse what went wrong there and try to increase overall performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting generally occurs when we have small number of training examples. One way to fix this problem is to augment our dataset so that it has sufficient number of training examples. Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better.\n",
    "\n",
    "In **tf.keras** we can implement this using the same **ImageDataGenerator** class we used before. We can simply pass  different transformations we would want to our dataset as a form of arguments and it will take care of applying it to the dataset during our training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Horizontal Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n",
    "                                               directory=train_dir,\n",
    "                                               shuffle=True,\n",
    "                                               target_size=(IMG_SHAPE,IMG_SHAPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are simply re-using the same custom plotting function \n",
    "# we defined and used above to visualize our training images\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly rotating the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at different augmentation called rotation and apply 45 degrees of rotation randomly to our training examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(rescale=1./255, rotation_range=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n",
    "                                               directory=train_dir,\n",
    "                                               shuffle=True, \n",
    "                                               target_size=(IMG_SHAPE, IMG_SHAPE))\n",
    "\n",
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply Zoom augmentation to our dataset to zoom images up to 50% randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(rescale=1./255, zoom_range=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n",
    "                                               directory=train_dir,\n",
    "                                               shuffle=True,\n",
    "                                               target_size=(IMG_SHAPE, IMG_SHAPE))\n",
    "\n",
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply all the augmentations we saw above and even more with just one line of code. We can simply pass the augmentations as arguments with proper values and that would be all.\n",
    "\n",
    "Here, we have applied rescale, rotation of 45 degrees, width shift, height shift, horizontal flip and zoom augmentation to our training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen_train = ImageDataGenerator(rescale=1./255,\n",
    "                                     rotation_range=45,\n",
    "                                     width_shift_range=.15,\n",
    "                                     height_shift_range=.15,\n",
    "                                     horizontal_flip=True,\n",
    "                                     zoom_range=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size,\n",
    "                                                     directory=train_dir,\n",
    "                                                     shuffle=True,\n",
    "                                                     target_size=(IMG_SHAPE,IMG_SHAPE),\n",
    "                                                     class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how a single image would look like 5 different times, when we pass these augmentations randomly to our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.evaluate_generator(test_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "print(\"loss value: {:.3f}\".format(history[0]))\n",
    "# accuracy\n",
    "print(\"accuracy value: {:.3f}\".format(history[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
